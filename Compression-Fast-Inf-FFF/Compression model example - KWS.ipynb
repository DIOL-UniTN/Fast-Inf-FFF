{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464058eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='leocus4', repo_name='TinyFFF', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a0e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT FOR COMPRESSION\n",
    "import sys\n",
    "sys.path.insert(0, './Compression-Fast-Inf-FFF/')\n",
    "from sparsecompFFF import compress_FF_models, print_size_model\n",
    "from save_torch_model_to_c import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad620fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = DEVICE\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbe9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36e4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b901a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfeedforward import FFF\n",
    "\n",
    "def train(net, trainloader, epochs, norm_weight=0.0):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the network for the given number of epochs\n",
    "    for _ in range(epochs):\n",
    "        # Iterate over data\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(images), labels)\n",
    "            if norm_weight != 0:\n",
    "                loss += norm_weight * net.fff.w1s.pow(2).sum()\n",
    "                loss += norm_weight * net.fff.w2s.pow(2).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    # Train the network for the given number of epochs\n",
    "    with torch.no_grad():\n",
    "        # Iterate over data\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_width, leaf_width, output_width, depth, dropout, region_leak):\n",
    "        super(Net, self).__init__()\n",
    "        self.fff = FFF(input_width, leaf_width, output_width, depth, torch.nn.ReLU(), dropout, train_hardened=True, region_leak=region_leak)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), -1)\n",
    "        x = self.fff(x)\n",
    "        x = torch.nn.functional.softmax(x, -1)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.fff.parameters()\n",
    "\n",
    "\n",
    "class FF(torch.nn.Module):\n",
    "    def __init__(self, input_width, layer_width, output_width):\n",
    "        super(FF, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_width, layer_width)\n",
    "        self.fc2 = torch.nn.Linear(layer_width, output_width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.softmax(self.fc2(x), -1)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [*self.fc1.parameters(), *self.fc2.parameters()]\n",
    "\n",
    "\n",
    "def compute_n_params(input_width: int, l_w: int, depth: int, output_width: int):\n",
    "    fff = Net(input_width, l_w, output_width, depth, 0, 0)\n",
    "    ff = FF(input_width, l_w, output_width)\n",
    "\n",
    "    n_ff = 0\n",
    "    n_fff = 0\n",
    "    for p in ff.parameters():\n",
    "        n_ff += p.numel()\n",
    "    for i, p in enumerate(fff.parameters()):\n",
    "        print(f\"[{i}-th layer]: {p.shape}\")\n",
    "        n_fff += p.numel()\n",
    "\n",
    "    print(f\"FFF: {n_fff}\\nFF: {n_ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e192bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_dist(net, testloader):\n",
    "    \"\"\"\n",
    "    Returns the distribution of samples throughout the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    y = []\n",
    "    l = []\n",
    "    with torch.no_grad():\n",
    "        # Iterate over data\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "            outputs, leaves = net.forward(images, return_nodes=True)\n",
    "            y.append(labels)\n",
    "            l.append(leaves)\n",
    "    y = torch.concat(y, 0)\n",
    "    l = torch.concat(l, 0)\n",
    "    return y, l\n",
    "\n",
    "\n",
    "class FFFWrapper(torch.nn.Module):\n",
    "    def __init__(self, fff):\n",
    "        super(FFFWrapper, self).__init__()\n",
    "        self._fff = fff\n",
    "        self._fastinference = [None for i in range(2 ** (self._fff.fff.depth.item()))]\n",
    "\n",
    "    def forward(self, x, return_nodes=False):\n",
    "        \"\"\"\n",
    "        Override the forward method in order to log the data distribution.\n",
    "        \"\"\"\n",
    "        x = x.view(len(x), -1)\n",
    "        original_shape = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        last_node = torch.zeros(len(x))\n",
    "\n",
    "        current_nodes = torch.zeros((batch_size,), dtype=torch.long, device=x.device)\n",
    "        for i in range(self._fff.fff.depth.item()):\n",
    "            plane_coeffs = self._fff.fff.node_weights.index_select(dim=0, index=current_nodes)\n",
    "            plane_offsets = self._fff.fff.node_biases.index_select(dim=0, index=current_nodes)\n",
    "            plane_coeff_score = torch.bmm(x.unsqueeze(1), plane_coeffs.unsqueeze(-1))\n",
    "            plane_score = plane_coeff_score.squeeze(-1) + plane_offsets\n",
    "            plane_choices = (plane_score.squeeze(-1) >= 0).long()\n",
    "\n",
    "            platform = torch.tensor(2 ** i - 1, dtype=torch.long, device=x.device)\n",
    "            next_platform = torch.tensor(2 ** (i+1) - 1, dtype=torch.long, device=x.device)\n",
    "            current_nodes = (current_nodes - platform) * 2 + plane_choices + next_platform\n",
    "\n",
    "        leaves = current_nodes - next_platform\n",
    "        new_logits = torch.empty((batch_size, self._fff.fff.output_width), dtype=torch.float, device=x.device)\n",
    "        last_node = leaves\n",
    "\n",
    "        for i in range(leaves.shape[0]):\n",
    "            leaf_index = leaves[i]\n",
    "            if self._fastinference[leaf_index] is not None:\n",
    "                new_logits[i] = self._fastinference[leaf_index]\n",
    "            else:\n",
    "                logits = torch.matmul( x[i].unsqueeze(0), self._fff.fff.w1s[leaf_index])\n",
    "                logits += self._fff.fff.b1s[leaf_index].unsqueeze(-2)\n",
    "                activations = self._fff.fff.activation(logits)\n",
    "                new_logits[i] = torch.matmul( activations, self._fff.fff.w2s[leaf_index]).squeeze(-2)\n",
    "\n",
    "        if return_nodes:\n",
    "            return new_logits.view(*original_shape[:-1], self._fff.fff.output_width), last_node\n",
    "        return new_logits.view(*original_shape[:-1], self._fff.fff.output_width)\n",
    "\n",
    "\n",
    "    def simplify_leaves(self, trainloader):\n",
    "        y, leaves = (get_dist(self, trainloader))\n",
    "        y = y.cpu().detach().numpy()\n",
    "        outputs = y.max() + 1\n",
    "        leaves = leaves.cpu().detach().numpy()\n",
    "\n",
    "        n_simplifications = 0\n",
    "        ratios = {}\n",
    "        for l in np.unique(leaves):\n",
    "            ratios[l] = torch.zeros(outputs)\n",
    "            indices = leaves == l\n",
    "\n",
    "            for i in range(outputs):\n",
    "                ratios[l][i] = (np.sum(y[indices] == i) / np.sum(indices))\n",
    "\n",
    "            argmax = np.argmax(ratios[l])\n",
    "            if ratios[l][argmax] > 0.7:\n",
    "                output = torch.zeros(outputs)\n",
    "                output[argmax] = 1\n",
    "                self._fastinference[l] = output\n",
    "                n_simplifications += 1\n",
    "                print(f\"Leaf {l} has been replaced with {argmax}\")\n",
    "        print(self._fastinference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fee5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFFWrapper(torch.nn.Module):\n",
    "    def __init__(self, fff):\n",
    "        super(FFFWrapper, self).__init__()\n",
    "        self._fff = fff\n",
    "        self._fastinference = [None for i in range(2 ** (self._fff.fff.depth.item()))]\n",
    "\n",
    "    def forward(self, x, return_nodes=False):\n",
    "        \"\"\"\n",
    "        Override the forward method in order to log the data distribution.\n",
    "        \"\"\"\n",
    "        x = x.view(len(x), -1)\n",
    "        original_shape = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        last_node = torch.zeros(len(x))\n",
    "\n",
    "        current_nodes = torch.zeros((batch_size,), dtype=torch.long, device=x.device)\n",
    "        for i in range(self._fff.fff.depth.item()):\n",
    "            plane_coeffs = self._fff.fff.node_weights.index_select(dim=0, index=current_nodes)\n",
    "            plane_offsets = self._fff.fff.node_biases.index_select(dim=0, index=current_nodes)\n",
    "            plane_coeff_score = torch.bmm(x.unsqueeze(1), plane_coeffs.unsqueeze(-1))\n",
    "            plane_score = plane_coeff_score.squeeze(-1) + plane_offsets\n",
    "            plane_choices = (plane_score.squeeze(-1) >= 0).long()\n",
    "\n",
    "            platform = torch.tensor(2 ** i - 1, dtype=torch.long, device=x.device)\n",
    "            next_platform = torch.tensor(2 ** (i+1) - 1, dtype=torch.long, device=x.device)\n",
    "            current_nodes = (current_nodes - platform) * 2 + plane_choices + next_platform\n",
    "\n",
    "        leaves = current_nodes - next_platform\n",
    "        new_logits = torch.empty((batch_size, self._fff.fff.output_width), dtype=torch.float, device=x.device)\n",
    "        last_node = leaves\n",
    "\n",
    "        for i in range(leaves.shape[0]):\n",
    "            leaf_index = leaves[i]\n",
    "            if self._fastinference[leaf_index] is not None:\n",
    "                new_logits[i] = self._fastinference[leaf_index]\n",
    "            else:\n",
    "                logits = torch.matmul( x[i].unsqueeze(0), self._fff.fff.w1s[leaf_index])\n",
    "                logits += self._fff.fff.b1s[leaf_index].unsqueeze(-2)\n",
    "                activations = self._fff.fff.activation(logits)\n",
    "                new_logits[i] = torch.matmul( activations, self._fff.fff.w2s[leaf_index]).squeeze(-2)\n",
    "\n",
    "        if return_nodes:\n",
    "            return new_logits.view(*original_shape[:-1], self._fff.fff.output_width), last_node\n",
    "        return new_logits.view(*original_shape[:-1], self._fff.fff.output_width)\n",
    "\n",
    "\n",
    "    def simplify_leaves(self, trainloader):\n",
    "        y, leaves = (get_dist(self, trainloader))\n",
    "        y = y.cpu().detach().numpy()\n",
    "        outputs = y.max() + 1\n",
    "        leaves = leaves.cpu().detach().numpy()\n",
    "\n",
    "        n_simplifications = 0\n",
    "        ratios = {}\n",
    "        for l in np.unique(leaves):\n",
    "            ratios[l] = torch.zeros(outputs)\n",
    "            indices = leaves == l\n",
    "\n",
    "            for i in range(outputs):\n",
    "                ratios[l][i] = (np.sum(y[indices] == i) / np.sum(indices))\n",
    "\n",
    "            argmax = np.argmax(ratios[l])\n",
    "            if ratios[l][argmax] > 0.7:\n",
    "                output = torch.zeros(outputs)\n",
    "                output[argmax] = 1\n",
    "                self._fastinference[l] = output\n",
    "                n_simplifications += 1\n",
    "                print(f\"Leaf {l} has been replaced with {argmax}\")\n",
    "        print(self._fastinference)\n",
    "    \n",
    "    def simplify_leaves_full(self, trainloader):\n",
    "        y, leaves = (get_dist(self, trainloader))\n",
    "        y = y.cpu().detach().numpy()\n",
    "        outputs = y.max() + 1\n",
    "        leaves = leaves.cpu().detach().numpy()\n",
    "\n",
    "        n_simplifications = 0\n",
    "        ratios = {}\n",
    "        for l in np.unique(leaves):\n",
    "            ratios[l] = torch.zeros(outputs)\n",
    "            indices = leaves == l\n",
    "\n",
    "            for i in range(outputs):\n",
    "                ratios[l][i] = (np.sum(y[indices] == i) / np.sum(indices))\n",
    "\n",
    "            argmax = np.argmax(ratios[l])\n",
    "            if ratios[l][argmax] >= 0:\n",
    "                output = torch.zeros(outputs)\n",
    "                output[argmax] = 1\n",
    "                self._fastinference[l] = output\n",
    "                n_simplifications += 1\n",
    "                print(f\"Leaf {l} has been replaced with {argmax}\")\n",
    "        print(self._fastinference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec2ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, fold=\"train\"):\n",
    "        df = pd.read_csv(f\"../MobiCom2024/features/sa_{fold}.csv\")\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for _, (dir, name, label) in df.iterrows():\n",
    "            self.data.append(np.load(f\"../MobiCom2024/features/{dir}/{name.replace('wav', 'npy')}\"))\n",
    "            self.labels.append(label)\n",
    "        self.data = np.array(self.data).astype(np.float32)\n",
    "        self.labels = np.array(self.labels).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cf7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SpeechDataset(\"train\")\n",
    "testset = SpeechDataset(\"test\")\n",
    "valset = SpeechDataset(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b9bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainset': 26982, 'valset': 3854, 'testset': 7710}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Select class to keep \n",
    "train_loader = DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=512)\n",
    "test_loader = DataLoader(testset, batch_size=512)\n",
    "\n",
    "num_examples = {\"trainset\" : len(trainset), \"valset\" : len(valset), \"testset\" : len(testset)}\n",
    "\n",
    "print(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c57c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_run = [\n",
    "'9cc6a99bbdaf47ea93157add2e3abff5',\n",
    "]\n",
    "\n",
    "# mlflow.artifacts.download_artifacts(run_id=run_id, dst_path=\".\")\n",
    "# wrapped_model = pickle.load(open(\"./truncated_model.pkl\", \"rb\"))\n",
    "# wrapped_model._fff.fff.depth.item()\n",
    "# wrapped_model._fff.fff.input_width\n",
    "# wrapped_model._fff.fff.leaf_width\n",
    "# wrapped_model._fff.fff.output_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc7c9ea1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72614b6ba6344792ad0d5f56535ecfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:\t 9cc6a99bbdaf47ea93157add2e3abff5\n",
      "Depth:\t 3\n",
      "Input:\t 793\n",
      "Output:\t 10\n",
      "Leaf:\t 32\n",
      "Buffer:\t 90\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range (0,len(list_of_run)):\n",
    "    run_id = list_of_run[i]\n",
    "    \n",
    "    run = mlflow.get_run(run_id)\n",
    "    starting_run_id = run.data.params['starting_run']\n",
    "    starting_run = mlflow.get_run(starting_run_id)\n",
    "    norm_weight = starting_run.data.params['norm_weight']\n",
    "    #print(norm_weight)\n",
    "    \n",
    "    mlflow.artifacts.download_artifacts(run_id=run_id, dst_path=\"./baselines\")\n",
    "    wrapped_model = pickle.load(open(\"./baselines/truncated_model.pkl\", \"rb\"))\n",
    "    depth = wrapped_model._fff.fff.depth.item()\n",
    "    input_width = wrapped_model._fff.fff.input_width\n",
    "    leaf_width = wrapped_model._fff.fff.leaf_width\n",
    "    output_width = wrapped_model._fff.fff.output_width\n",
    "    buffer_size = 2*(leaf_width + output_width + 3)\n",
    "    print(\"Run:\\t\", run_id)\n",
    "    print(\"Depth:\\t\", depth)\n",
    "    print(\"Input:\\t\", input_width)\n",
    "    print(\"Output:\\t\", output_width)\n",
    "    print(\"Leaf:\\t\", leaf_width)\n",
    "    print(\"Buffer:\\t\", buffer_size)\n",
    "    \n",
    "    # to reduce the sparsity and train only below a certain tresholds\n",
    "    list_of_sizes = [100, 90, 80, 70, 60, 50]\n",
    "    checked_sizes = [False for x in list_of_sizes]\n",
    "    current_size_index = 0\n",
    "    \n",
    "    start = 0.5\n",
    "    a = start\n",
    "    b = start\n",
    "    sizes=[]\n",
    "    before_trunc_sizes=[]\n",
    "    trunc_sizes=[]\n",
    "\n",
    "    model = wrapped_model.to(device)\n",
    "    \n",
    "    layers_list = []\n",
    "    for i, (name, p) in enumerate(model.named_parameters()):\n",
    "        if (len(list(p.shape)) > 1 and p.requires_grad):\n",
    "            layers_list.append(p)\n",
    "    un, comp, layers = print_size_model(model, layers_list, [1,1,1,1,1,1])\n",
    "    \n",
    "    result.append({'run_id': run_id, 'depth': depth,'input_width':input_width, \n",
    "                   'output': output_width, 'leaf_width':leaf_width,\n",
    "                   'buffer_size': buffer_size, 'sizes': un})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5777cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_id': '9cc6a99bbdaf47ea93157add2e3abff5', 'depth': 3, 'input_width': 793, 'output': 10, 'leaf_width': 32, 'buffer_size': 90, 'sizes': 845856}\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b81494a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size requested: 50 KB\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1]\n",
      "Starting size of the model: 845.856 KB\n",
      "1 iteration -  Size: 764684.8 [1, 1, 0.45, 1, 1, 1]\n",
      "2 iteration -  Size: 691601.92 [1, 1, 0.405, 1, 1, 1]\n",
      "3 iteration -  Size: 625827.3280000001 [1, 1, 0.36450000000000005, 1, 1, 1]\n",
      "4 iteration -  Size: 566630.1952000001 [1, 1, 0.32805000000000006, 1, 1, 1]\n",
      "5 iteration -  Size: 513352.7756800001 [1, 1, 0.29524500000000004, 1, 1, 1]\n",
      "6 iteration -  Size: 465403.09811200004 [1, 1, 0.2657205, 1, 1, 1]\n",
      "7 iteration -  Size: 422248.3883008 [1, 1, 0.23914845, 1, 1, 1]\n",
      "8 iteration -  Size: 383409.14947072 [1, 1, 0.215233605, 1, 1, 1]\n",
      "9 iteration -  Size: 348453.83452364797 [1, 1, 0.1937102445, 1, 1, 1]\n",
      "10 iteration -  Size: 316994.0510712832 [1, 1, 0.17433922005, 1, 1, 1]\n",
      "11 iteration -  Size: 288680.2459641549 [1, 1, 0.156905298045, 1, 1, 1]\n",
      "12 iteration -  Size: 263197.82136773935 [1, 1, 0.1412147682405, 1, 1, 1]\n",
      "13 iteration -  Size: 240263.63923096546 [1, 1, 0.12709329141645, 1, 1, 1]\n",
      "14 iteration -  Size: 219622.8753078689 [1, 1, 0.114383962274805, 1, 1, 1]\n",
      "15 iteration -  Size: 201046.18777708203 [1, 1, 0.10294556604732451, 1, 1, 1]\n",
      "16 iteration -  Size: 184327.16899937383 [1, 1, 0.09265100944259205, 1, 1, 1]\n",
      "17 iteration -  Size: 169280.05209943643 [1, 1, 0.08338590849833284, 1, 1, 1]\n",
      "18 iteration -  Size: 155737.6468894928 [1, 1, 0.07504731764849956, 1, 1, 1]\n",
      "19 iteration -  Size: 143549.4822005435 [1, 1, 0.0675425858836496, 1, 1, 1]\n",
      "20 iteration -  Size: 132580.13398048916 [1, 1, 0.06078832729528464, 1, 1, 1]\n",
      "21 iteration -  Size: 122707.72058244023 [1, 1, 0.054709494565756175, 1, 1, 1]\n",
      "22 iteration -  Size: 113822.54852419622 [1, 1, 0.049238545109180555, 1, 1, 1]\n",
      "23 iteration -  Size: 105825.89367177659 [1, 1, 0.0443146905982625, 1, 1, 1]\n",
      "24 iteration -  Size: 98628.90430459892 [1, 1, 0.03988322153843625, 1, 1, 1]\n",
      "25 iteration -  Size: 92151.61387413903 [1, 1, 0.03589489938459262, 1, 1, 1]\n",
      "26 iteration -  Size: 86322.05248672512 [1, 1, 0.03230540944613336, 1, 1, 1]\n",
      "27 iteration -  Size: 81075.44723805261 [1, 1, 0.029074868501520024, 1, 1, 1]\n",
      "28 iteration -  Size: 76353.50251424735 [1, 1, 0.026167381651368022, 1, 1, 1]\n",
      "29 iteration -  Size: 72103.75226282261 [1, 1, 0.023550643486231218, 1, 1, 1]\n",
      "30 iteration -  Size: 68278.97703654037 [1, 1, 0.021195579137608098, 1, 1, 1]\n",
      "31 iteration -  Size: 64836.67933288632 [1, 1, 0.019076021223847286, 1, 1, 1]\n",
      "32 iteration -  Size: 61738.61139959769 [1, 1, 0.017168419101462558, 1, 1, 1]\n",
      "33 iteration -  Size: 58950.35025963792 [1, 1, 0.015451577191316302, 1, 1, 1]\n",
      "34 iteration -  Size: 56440.915233674124 [1, 1, 0.013906419472184673, 1, 1, 1]\n",
      "35 iteration -  Size: 54182.42371030671 [1, 1, 0.012515777524966205, 1, 1, 1]\n",
      "36 iteration -  Size: 52149.78133927604 [1, 1, 0.011264199772469584, 1, 1, 1]\n",
      "37 iteration -  Size: 50320.40320534844 [1, 1, 0.010137779795222625, 1, 1, 1]\n",
      "Disabling: _fff.fff.depth\n",
      "Disabling: _fff.fff.node_weights\n",
      "Disabling: _fff.fff.node_biases\n",
      "Disabling: _fff.fff.w1s\n",
      "Disabling: _fff.fff.b1s\n",
      "Disabling: _fff.fff.w2s\n",
      "Disabling: _fff.fff.b2s\n",
      "Activating: torch.Size([7, 793])\n",
      "Activating: torch.Size([7, 1])\n",
      "Activating: torch.Size([8, 793, 32])\n",
      "Activating: torch.Size([8, 32])\n",
      "Activating: torch.Size([8, 32, 10])\n",
      "Activating: torch.Size([8, 10])\n",
      "Epoch [1/8], Step[53/53], Loss: 1.2145\n",
      "Epoch[1]: v_loss: 1.36047 v_acc: 60.0\n",
      "Epoch [2/8], Step[53/53], Loss: 1.1599\n",
      "Epoch[2]: v_loss: 1.11838 v_acc: 65.22698\n",
      "Epoch [3/8], Step[53/53], Loss: 0.9158\n",
      "Epoch[3]: v_loss: 1.01452 v_acc: 67.58755\n",
      "Epoch [4/8], Step[53/53], Loss: 0.8101\n",
      "Epoch[4]: v_loss: 0.9548 v_acc: 69.3904\n",
      "Epoch [5/8], Step[53/53], Loss: 0.6796\n",
      "Epoch[5]: v_loss: 0.9202 v_acc: 70.33722\n",
      "Epoch [6/8], Step[53/53], Loss: 0.7633\n",
      "Epoch[6]: v_loss: 0.89556 v_acc: 70.97276\n",
      "Epoch [7/8], Step[53/53], Loss: 0.7781\n",
      "Epoch[7]: v_loss: 0.87579 v_acc: 71.72503\n",
      "Epoch [8/8], Step[53/53], Loss: 0.7680\n",
      "Epoch[8]: v_loss: 0.86129 v_acc: 72.19196\n",
      "Best model saved at epoch:  8\n",
      "Best acc model saved at epoch:  8\n",
      "Accuracy of the network on the 10000 test images: 72.19195849546044 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48673.96288481359"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_FF_models(model, 50, train_loader, test_loader, val_loader=val_loader, num_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e608b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size requested: 40 KB\n",
      "Starting Density of model's parameters: [1, 1, 0.00912771910466581, 1, 1, 1]\n",
      "Starting size of the model: 48.64799999999997 KB\n",
      "1 iteration -  Size: 49635.59999999998 [0.45, 1, 0.00912771910466581, 1, 1, 1]\n",
      "2 iteration -  Size: 47637.239999999976 [0.405, 1, 0.00912771910466581, 1, 1, 1]\n",
      "3 iteration -  Size: 45838.71599999997 [0.36450000000000005, 1, 0.00912771910466581, 1, 1, 1]\n",
      "4 iteration -  Size: 44220.04439999998 [0.32805000000000006, 1, 0.00912771910466581, 1, 1, 1]\n",
      "5 iteration -  Size: 42763.23995999998 [0.29524500000000004, 1, 0.00912771910466581, 1, 1, 1]\n",
      "6 iteration -  Size: 41452.115963999975 [0.2657205, 1, 0.00912771910466581, 1, 1, 1]\n",
      "7 iteration -  Size: 40272.104367599975 [0.23914845, 1, 0.00912771910466581, 1, 1, 1]\n",
      "Disabling: _fff.fff.depth\n",
      "Disabling: _fff.fff.node_weights\n",
      "Disabling: _fff.fff.node_biases\n",
      "Disabling: _fff.fff.w1s\n",
      "Disabling: _fff.fff.b1s\n",
      "Disabling: _fff.fff.w2s\n",
      "Disabling: _fff.fff.b2s\n",
      "Activating: torch.Size([7, 793])\n",
      "Activating: torch.Size([7, 1])\n",
      "Activating: torch.Size([8, 793, 32])\n",
      "Activating: torch.Size([8, 32])\n",
      "Activating: torch.Size([8, 32, 10])\n",
      "Activating: torch.Size([8, 10])\n",
      "Epoch [1/8], Step[53/53], Loss: 0.7529\n",
      "Epoch[1]: v_loss: 0.91467 v_acc: 70.45396\n",
      "Epoch [2/8], Step[53/53], Loss: 0.9635\n",
      "Epoch[2]: v_loss: 0.99344 v_acc: 69.16991\n",
      "Epoch [3/8], Step[53/53], Loss: 1.4623\n",
      "Epoch[3]: v_loss: 1.50221 v_acc: 46.67964\n",
      "Epoch [4/8], Step[53/53], Loss: 1.3103\n",
      "Epoch[4]: v_loss: 1.39924 v_acc: 53.20363\n",
      "Epoch [5/8], Step[53/53], Loss: 1.2471\n",
      "Epoch[5]: v_loss: 1.24135 v_acc: 59.68872\n",
      "Epoch [6/8], Step[53/53], Loss: 1.6452\n",
      "Epoch[6]: v_loss: 1.61245 v_acc: 42.80156\n",
      "Epoch [7/8], Step[53/53], Loss: 1.3605\n",
      "Epoch[7]: v_loss: 1.41607 v_acc: 52.84047\n",
      "Epoch [8/8], Step[53/53], Loss: 1.8440\n",
      "Epoch[8]: v_loss: 1.7304 v_acc: 38.7808\n",
      "Best model saved at epoch:  1\n",
      "Best acc model saved at epoch:  1\n",
      "Accuracy of the network on the 10000 test images: 70.45395590142672 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39210.093930839976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_FF_models(model, 40, train_loader, test_loader, val_loader=val_loader, fastInfLoss=True, fastInfNormWeight=0.0001, num_epochs=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14dedf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_fastinference = str([-1 if x is None else int(x.argmax()) for x in model._fastinference])\n",
    "main(model, \"model_to_c\", original_fastinference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d1ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
